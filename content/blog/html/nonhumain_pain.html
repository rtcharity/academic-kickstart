<p><em>This essay project was written by Jason Schukraft, with contributions from Peter Hurford, Max Carpendale, and Marcus A. Davis.</em></p>

<p>Other humans merit moral concern. We think many nonhumans merit moral concern too. But how do we know? And which nonhumans? Chimpanzees? Chickens? Bumblebees? Protozoa? Roombas? Rocks? Where and how do we draw a line?</p>

<p>What would it take to justifiably believe that some nonhuman experiences pain (or pleasure) in a morally significant way?<a href="#fn:1" id="fnref:1" title="see footnote" class="footnote"><sup>1</sup></a> This is a tough question, but it is incredibly important to get right. Humans constitute a very, <em>very</em> <a href="https://reducing-suffering.org/how-many-wild-animals-are-there/">small fraction</a> of the animal kingdom. If other vertebrate animals experience morally significant pain, then much of our <a href="https://mercyforanimals.org/the-problem">engagement</a> with these animals is deeply immoral. If invertebrate animals experience morally significant pain, then, given the sheer number of invertebrates,<a href="#fn:2" id="fnref:2" title="see footnote" class="footnote"><sup>2</sup></a> an almost incomprehensible amount of morally significant <a href="https://foundational-research.org/the-importance-of-wild-animal-suffering/">suffering</a> occurs beyond the ken of normal human attention. And if the capacity to experience morally significant pain is not restricted to organic entities, then human civilizations of the future may be capable of producing <a href="https://foundational-research.org/risks-of-astronomical-future-suffering/#Sentient_simulations">exponentially more sentient entities</a> than presently exist.</p>

<p>On the other hand, if many, most, or all nonhumans do not experience morally significant pain, then it could be a waste of resources to try to change their condition. Given that there are millions of humans currently experiencing morally significant pain (for whom <a href="https://www.againstmalaria.com/">these resources would be a great aid</a>), the opportunity cost of wasting time, talent, and money on nonhumans appears tremendous.</p>

<p>Figuring out where and whether to allocate resources to help nonhumans is of significant interest to Rethink Priorities. This post is our first in a series on morally significant pain in invertebrates. We focus on invertebrates for two reasons: (1) We are already reasonably confident that mammals, birds, reptiles, amphibians, and most fish feel morally significant pain,<a href="#fn:3" id="fnref:3" title="see footnote" class="footnote"><sup>3</sup></a> and hence must be included in our moral calculations, but we are unsure if more distantly related animals warrant similar concern, and (2) The subject of invertebrate sentience, though recently gaining traction both in the <a href="https://academic.oup.com/ilarjournal/article/52/2/175/659957">scientific literature</a> and the <a href="https://was-research.org/writing-by-others/reducing-suffering-amongst-invertebrates-insects/">effective altruism community</a>, appears neglected relative to the subject’s potential import. In future posts we look at which features might be relevant for determining whether an entity is capable of experiencing pain. We also present a detailed table outlining the distribution of these features throughout the animal kingdom.<a href="#fn:4" id="fnref:4" title="see footnote" class="footnote"><sup>4</sup></a></p>

<p>Of course, we recognize that delineating the phylogenetic distribution of morally significant pain is an extraordinarily complex and difficult task, one that we are extremely unlikely to solve outright. To put it mildly, much more research, at virtually every level of the problem, is needed. Nevertheless, the urgency of the issue compels us to address it now, before all the potentially relevant evidence is in. As grantmakers and charity entrepreneurs, we do not have the luxury to wait. We must decide how to allocate resources now, in our current epistemically incomplete state. Our goal in this series of posts is to determine, to the best of our abilities and within reasonable funding and time constraints, what we should think about morally significant pain in invertebrates, given the current state of the evidence.</p>

<p>To that end, we begin with a review of the philosophical difficulties inherent in the detection of morally significant pain in nonhumans. We discuss eight conceptually sequential steps, alongside their attendant difficulties, needed to identify morally significant pain in nonhumans.<a href="#fn:5" id="fnref:5" title="see footnote" class="footnote"><sup>5</sup></a> The first three steps concern detecting pain; the other five steps concern determining whether (and to what extent) the pain is morally significant.<a href="#fn:6" id="fnref:6" title="see footnote" class="footnote"><sup>6</sup></a></p>

<h1 id="theproblemofotherminds"><strong>The Problem of Other Minds</strong></h1>

<p>Start with yourself. You experience pleasure and pain. You can be as confident of this fact as you can be of any fact. Why? You have direct introspective access to at least some of your phenomenal states. But there is an asymmetry between you and everything else. You cannot know by direct introspection that someone else is phenomenally conscious.<a href="#fn:7" id="fnref:7" title="see footnote" class="footnote"><sup>7</sup></a> If you are justified in believing that other entities experience pains and pleasures, it must be by some different epistemic strategy. Solipsism is the view that one’s mind is the only mind that exists.<a href="#fn:8" id="fnref:8" title="see footnote" class="footnote"><sup>8</sup></a> If we are to justifiably believe that some nonhuman experiences pain, we must first overcome the challenge of solipsism.</p>

<p>Although philosophers disagree about the appropriate resolution, robust solipsism has few, if any, contemporary defenders. The idea that other humans experience pleasure and pain is very central to our web of beliefs. Any theory that would wage war against such a central belief had better come loaded with powerful ammunition. It is generally held that traditional arguments in favor of solipsism are incapable of providing such ammunition.<a href="#fn:9" id="fnref:9" title="see footnote" class="footnote"><sup>9</sup></a></p>

<h1 id="analogicalargumentandinferencetothebestexplanation"><strong>Analogical Argument and Inference to the Best Explanation</strong></h1>

<p>The most common response to solipsism takes the form of an <em>inference to the best explanation</em>.<a href="#fn:10" id="fnref:10" title="see footnote" class="footnote"><sup>10</sup></a> One begins with an examination of one’s own behavior. For example: when I cut my hand, I cry out, I move my hand away from the sharp object, and I later treat the wound with a clean bandage. Then one considers the behavior of other humans: they also cry out when cut and attend in similar ways to similar wounds.<a href="#fn:11" id="fnref:11" title="see footnote" class="footnote"><sup>11</sup></a> There are a variety of hypotheses which, if true, could explain this behavior. Perhaps they are sophisticated robots programmed to behave as I do. But the simplest and best explanation of the behavior of other humans is that they feel pain like I do.<a href="#fn:12" id="fnref:12" title="see footnote" class="footnote"><sup>12</sup></a></p>

<p>Of course, this explanation might be mistaken, and we might come to know it is mistaken. If I examined the heads of many fellow humans and in each case found not a brain but a crude artificial device receiving signals from a robotics factory, that would constitute a <em>defeater</em> for my prior explanation. I would then no longer be able to rationally endorse the view that other humans have mental states like I do. Inference to the best explanation tells us that, <em>in the absence of defeaters</em>, we are licensed to prefer the simplest explanation of a phenomenon.<a href="#fn:13" id="fnref:13" title="see footnote" class="footnote"><sup>13</sup></a></p>

<p>Inference to the best explanation is related to, but distinct from, <em>argument by analogy</em>. The basic structure of an analogical argument is as follows (where E1 is the source domain and E2 is the target domain):</p>

<p>(1) Entity E1 has some properties P1 … Pn<br />
(2) Entity E2 has the same properties P1 … Pn<br />
(3) Entity E1 has some further property Pn+1<br />
∴<br />
(4) Therefore, entity E2 likely has the same property Pn+1</p>

<p>Analogical arguments are by their nature inductive. The wider the inferential base upon which to base an induction, the better the inductive argument. But pain is a private mental state, so when it comes to pain, we each have an inductive base of one (namely, ourselves). Inductive inferences from an inductive base of one generally aren&#8217;t sound. So we probably don’t know that others experience pain by analogical reasoning alone.<a href="#fn:14" id="fnref:14" title="see footnote" class="footnote"><sup>14</sup></a></p>

<p>Inference to the best explanation, by contrast, is abductive. Abductive arguments are non-deductive like traditional inductive arguments, but, unlike traditional inductive arguments which are, allegedly, justified empirically, abductive arguments are justified a priori. We are justified in using induction because, as a matter of contingent fact, induction has worked well in the past.<a href="#fn:15" id="fnref:15" title="see footnote" class="footnote"><sup>15</sup></a> Instances of abductive reasoning, in contrast, are generally held to instantiate principles of rationality, which, if they are known at all, are known a priori.</p>

<p>Inference to the best explanation can also be applied to nonhumans.<a href="#fn:16" id="fnref:16" title="see footnote" class="footnote"><sup>16</sup></a> If a class of nonhumans exhibits pain behavior<a href="#fn:17" id="fnref:17" title="see footnote" class="footnote"><sup>17</sup></a> sufficiently similar to humans, then, in the absence of defeaters, we are licensed to prefer the explanation that they feel pain to alternate explanations. But what counts as sufficiently similar? And what counts as a defeater?</p>

<p>Consider similarity first. One worry is that the behavior of phylogenetically distant animals (to say nothing of inorganic entities) is so alien that the behavior cannot even be accurately described without resorting to problematic anthropomorphizing. Even when we <em>can</em> accurately describe the behavior of, say, invertebrates without inappropriately anthropomorphizing them, it’s unclear how much similarity antecedently to expect. Different species of animal, after all, are <em>different</em>. To take a trivial example: most of the time, when humans are in pain, they grimace. But the hard exoskeleton of an insect does not allow for grimacing. Does this difference provide a small bit of evidence that insects don’t feel pain? Presumably not. But that doesn’t mean that grimacing is irrelevant. Consider another example: many times when a human is in pain, she cries out. Again, owing to anatomical differences, we shouldn’t expect this feature to be widespread in invertebrates, even if they do feel pain. But farm animal vocalization has recently been taken to be <a href="https://www.sciencedirect.com/science/article/abs/pii/S0168159104000565?via%3Dihub">a good metric of animal welfare</a> in pigs, cows, and chickens.</p>

<p>The general lesson here is that there is no set of features which is universally relevant for the detection of pain in nonhumans. Even if pain experiences are widespread throughout the animal kingdom, the extreme diversity of living organisms suggests that pain experiences might be expressed in behaviorally and neurobiologically distinct ways.</p>

<p>The same problem applies to potential defeaters. It was once widely thought that a neocortex is required for conscious experience.<a href="#fn:18" id="fnref:18" title="see footnote" class="footnote"><sup>18</sup></a> Thus, it was thought, any creature which lacked a neocortex thereby lacked conscious experience.<a href="#fn:19" id="fnref:19" title="see footnote" class="footnote"><sup>19</sup></a> No matter how similar the behavior, the absence of a neocortex in a creature served as a <em>defeater</em> for the view that that creature experienced pain.</p>

<p>Today the picture is more complicated. For starters, evidence is emerging that, <em>even in humans</em>, <a href="https://doi.org/10.1017/S0140525X07000891">a neocortex is not required for conscious experience</a>.<a href="#fn:20" id="fnref:20" title="see footnote" class="footnote"><sup>20</sup></a> More importantly, the absence of a neocortex doesn’t imply that there aren’t <a href="https://doi.org/10.1038/nrn1606">homologous cells performing the same role in other creatures</a>.<a href="#fn:21" id="fnref:21" title="see footnote" class="footnote"><sup>21</sup></a> The point to appreciate here is that the bar for justifiably believing that some neurological feature is a <em>necessary</em> condition on conscious experience is quite high. Neurological differences surely are relevant, but, in the absence of a general theory of consciousness, the degree to which they can be decisive is limited.</p>

<p>There is a further, more fundamental limitation to investigating consciousness empirically. Although pain states are associated (at least in humans) with various physiological responses, such as elevated heartbeat and increased respiration, pain cannot be <em>defined</em> in terms of these responses. It’s natural to suppose that my experience of pain <em>explains</em> why my heart starts beating faster and my respiration quickens. If pain just <em>is</em> elevated heartbeat and increased respiration (or whatever other physiological responses one favors), then we lose this natural explanation. More importantly, if we define pain in purely physiological terms, we miss the moral significance of pain. Pain is intrinsically morally bad (ceteris paribus) not because it causes or is identical to certain physiological responses. Pain is bad because it <em>feels</em> bad. Measured physical responses can provide evidence that an entity experiences the felt badness of pain, and that evidence can be decisive, but we should not confuse evidence of a phenomenon with the phenomenon itself. To investigate the phenomenon of consciousness directly, we probably ought to turn to philosophy.</p>

<h1 id="applyingageneraltheoryofconsciousness"><strong>Applying a General Theory of Consciousness</strong></h1>

<p>Determining whether an entity is phenomenally conscious is probably not a strictly scientific endeavor. At some point, some difficult philosophical theorizing might be needed to help us make appropriate attributions of consciousness. So suppose all the empirical data is in, and it’s still unclear whether a certain nonhuman is conscious. To settle the question, it would be nice to appeal to a well-justified general theory of consciousness. Below, I briefly examine three broad families of views about the relationship between mind and matter: dualism, physicalism, and a hybrid theory. But first, I outline some peculiar initial difficulties we face before embarking on a quest for a theory of mind. Collectively, these subsections show that uncertainty in philosophy of mind will at some point probably infect our credences about which nonhumans experience morally significant pain.</p>

<h2 id="thecommongroundproblem"><strong>The Common Ground Problem</strong></h2>

<p>Theories of consciousness, like all philosophical theories, begin with certain pre-theoretic intuitions about the subject matter. These pre-theoretic intuitions include background framework assumptions about roughly how widespread consciousness is likely to be across phyla. If final theories begin with radically different starting assumptions, comparing the theories won’t really be helpful. There has to be sufficient common ground in order for robust theory-comparison to be possible. But there’s some evidence that this common ground is lacking in theories of consciousness.<a href="#fn:22" id="fnref:22" title="see footnote" class="footnote"><sup>22</sup></a> Existing theories of consciousness, from the world’s top researchers, span the board from so-called “higher-order theories,” which, due to their metarepresentational requirements on consciousness, seem to deny consciousness to babies and dogs, to panpsychism, which attributes some degree of consciousness not just to plants and unicellular organisms but also to protons and electrons. One might have thought that these consequences could serve as reductios on their respective theories, but apparently this is not the case.<a href="#fn:23" id="fnref:23" title="see footnote" class="footnote"><sup>23</sup></a> So the field must allow an unusually diverse range of initial assumptions.<a href="#fn:24" id="fnref:24" title="see footnote" class="footnote"><sup>24</sup></a> This makes adjudicating between competing theories in philosophy of mind particularly hard.</p>

<h2 id="thecausalstatusofconsciousness:dualism"><strong>The Causal Status of Consciousness: Dualism</strong></h2>

<p>Every theory of consciousness must grapple with the causal status of consciousness. Epiphenomenalism is the view that mental events are causally inert. According to the epiphenomenalist, pains and pleasures exist, but they are nonphysical states. Because the physical world is causally closed, these nonphysical states have no causal power.<a href="#fn:25" id="fnref:25" title="see footnote" class="footnote"><sup>25</sup></a> All conscious experience could be subtracted from the world, and it would not make any physical difference.</p>

<p>According to epiphenomenalism, conscious experience doesn’t have a causal profile. If conscious experience doesn’t have a causal profile, then empirically investigating features which are allegedly indicative of conscious experience is probably a waste of time. If I cut my finger and cry out immediately thereafter, my cry is not <em>caused</em> by an experience of pain. So my cry is not <em>evidence</em> of pain, at least not in the straightforward way we normally take it to be.<a href="#fn:26" id="fnref:26" title="see footnote" class="footnote"><sup>26</sup></a> The same goes for more complicated physical features, such as brain size, opiate sensitivity, or long-term behavior modification to avoid noxious stimuli.</p>

<p>One motivation for epiphenomenalism is intuitions about so-called “phenomenal zombies.”<a href="#fn:27" id="fnref:27" title="see footnote" class="footnote"><sup>27</sup></a> A phenomenal zombie is a creature whose behavior and physical structure, down to the atomic level, is identical to that of a normal human being but who lacks any conscious experience. David Chalmers claims that phenomenal zombies are physically possible.<a href="#fn:28" id="fnref:28" title="see footnote" class="footnote"><sup>28</sup></a> If phenomenal zombies are physically possible, then, even if they are nonactual, mental states must be causally inert.</p>

<p>Epiphenomenalism is a natural consequence of many dualistic theories of mind.<a href="#fn:29" id="fnref:29" title="see footnote" class="footnote"><sup>29</sup></a> It seems true that anything that can cause a physical event must itself be a physical event. There are also arguments to the effect that <a href="https://plato.stanford.edu/entries/dualism/">mental states are nonphysical</a>. If those two claims are true, epiphenomenalism seems nigh on inevitable.</p>

<p>If epiphenomenalism is true, it will be very difficult, if not impossible, to determine whether an entity (aside from oneself) is phenomenally conscious. Certainly no amount of empirical information will settle the question. A complete account of the psychophysical laws could perhaps do the trick, but it’s unclear how we could come to justifiably believe that we have such an account. Relatedly, epiphenomenalism seems to undercut the force of the inference to the best explanation strategy for responding to solipsism. If epiphenomenalism is true, then mental states do <em>not</em> explain the behavior of other humans. At best I can infer that other humans have <em>brain states</em> similar to mine, but I am no longer justified in supposing that they are conscious.</p>

<h2 id="emergentism:ahybridtheory"><strong>Emergentism: A Hybrid Theory</strong></h2>

<p>Emergent properties are constituted by more fundamental entities yet are novel or irreducible with respect to them. (In simpler terms, the whole is greater than the sum of its parts.) <em>Emergentism</em> is a position in philosophy of mind that seeks to preserve the intuition that mental events and physical events are distinct without entailing epiphenomenalism. On this view, consciousness is an emergent property of the brain. Sometimes this point is put in epistemic terms: consciousness supervenes on constituent parts of the brain, but complete knowledge of all the brain’s constituent parts would not enable us to justifiably infer the existence of consciousness. (If we could so infer, then consciousness would be <em>reducible</em> to brain states, not emergent from them.)<a href="#fn:30" id="fnref:30" title="see footnote" class="footnote"><sup>30</sup></a></p>

<p>Emergentism leaves us in a better epistemic position than epiphenomenalism. Because mental states and functional brain states are necessarily connected (phenomenal zombies are physically impossible on this view), we can potentially employ inference to the best explanation to determine whether some nonhuman is conscious. Still, it’s not clear how well emergentism fundamentally avoids the problem of epiphenomenalism. According to the emergentist, although mental states and brain states are necessarily connected, they are metaphysically distinct: no amount of neuroscientific knowledge could explain how the brain gives rise to consciousness. The connection between brain states and mental states has, in the words of Hempel and Oppenheim (1948), “a mysterious quality of <a href="https://doi.org/10.1086/286983">absolute unexplainability</a>.”<a href="#fn:31" id="fnref:31" title="see footnote" class="footnote"><sup>31</sup></a> Of course, just because a phenomenon cannot be explained in terms of neuroscience doesn’t mean that the phenomenon can’t be explained at all. It may be possible to explain how the brain gives rise to consciousness in terms of substantive <a href="http://hdl.handle.net/2027/spo.3521354.0017.011">principles of metaphysical grounding</a>. Unfortunately, these principles seem as difficult to ascertain as the psychophysical laws that the epiphenomenalist purports to exist. Thus, this view seems to leave us in a similarly problematic epistemic position.</p>

<h2 id="semanticindeterminacy:physicalism"><strong>Semantic Indeterminacy: Physicalism</strong></h2>

<p>In contrast to the nonreductive emergentism outlined above, reductive physical accounts of the mind hold that mental states straightforwardly reduce to physical states. Although there are many <a href="https://plato.stanford.edu/entries/physicalism/#CasAgaPhyIQuaCon">arguments against reductive physicalism</a>, rehearsing them here is less helpful than considering what is implied by the <em>truth</em> of the view.</p>

<p>Consciousness, even if it is a purely physical feature of the world, is not a simple phenomenon. It’s unlikely that we will learn that consciousness reduces to a single feature of the brain. It’s much more plausible to suppose that consciousness is some complex bundle of physical features. Given the complexity of consciousness, it’s also implausible to suppose that we will be able to describe consciousness in terms of necessary and sufficient conditions. If reductive physicalism is true, consciousness is much more likely to be a <a href="http://itisonlyatheory.blogspot.com/2010/01/cluster-concepts.html">cluster concept</a>. Finally, it seems implausible that these features would be coextensive across the animal kingdom. Some entities, such as humans, might possess all the features. Some entities, such as plants, might possess none of the features. And some entities, such as <a href="https://en.wikipedia.org/wiki/Anaspidea">sea hares</a>, might possess some but not all of the features.<a href="#fn:32" id="fnref:32" title="see footnote" class="footnote"><sup>32</sup></a> Thus, if reductive physicalism is true, then at some point on the phylogenetic tree, it will probably be semantically indeterminate whether a given species is conscious. We might know all the physical facts about the species <em>and</em> know the correct theory of mind, and yet <em>still</em> not be able to say definitively whether a creature is conscious. This raises a difficult question: what is the moral status of a creature for whom it is semantically indeterminate that that creature is conscious?</p>

<h1 id="theunpleasantnessofpain"><strong>The Unpleasantness of Pain</strong></h1>

<p>I turn now away from the question of whether some nonhumans experience to pain to the question of the <em>moral</em> <em>significance</em> of that pain, supposing it exists. As we’ll see, we need not think that all pains are equally morally significant. Indeed, we might reasonably conclude that some pains ought to be ignored completely in our moral calculations.</p>

<p>Suppose we assign some moderately high credence to the claim that certain nonhumans, octopuses say, experience pain. What might these pain-experiences be like? In particular, we would want to know whether octopuses experience the <em>unpleasantness</em> of pain. It might seem like an analytic truth that pain is unpleasant, but there is actually good empirical evidence to suggest this is not necessarily so. Humans with <a href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-79948-3_762">pain asymbolia</a> report experiencing pain without the pain being unpleasant. This dissociation can also be induced pharmacologically, notably with <a href="https://books.google.com/books?id=x2Aqw1527ecC&amp;pg=PA33&amp;lpg=PA33&amp;dq=%22pain+dissociation%22+morphine&amp;source=bl&amp;ots=4euOptXoES&amp;sig=5AjdaHeN_Q8qBevTujgrgucndW4&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwioj83QwY7fAhUJUK0KHcwFDiMQ6AEwCHoECAIQAQ#v=onepage&amp;q=%22pain%20dissociation%22%20morphine&amp;f=false">morphine</a>.<a href="#fn:33" id="fnref:33" title="see footnote" class="footnote"><sup>33</sup></a></p>

<p>It’s possible that pain asymbolia patients are conceptually confused and that pain is <em>necessarily</em> unpleasant. But it’s also possible that pain is a multi-dimensional experience, the unpleasantness of which is only one dimension. Because the unpleasantness of pain almost always accompanies the other dimensions, we may be misled into thinking the various dimensions of pain are necessarily coextensive. To analogize: one might have thought that pains had to be localized in some part of one’s body, at least vaguely so. But <a href="https://dx.doi.org/10.1155%2F2011%2F864605">phantom limb pain</a> shows that this is not the case.</p>

<p>The unpleasantness of pain is what makes pain experiences non-instrumentally bad.<a href="#fn:34" id="fnref:34" title="see footnote" class="footnote"><sup>34</sup></a> Thus, pain experiences may not be morally significant <em>simplicter</em>. They may be morally significant only when they are accompanied by the usual (in humans, at least) negatively valenced phenomenology.</p>

<p>Accounting for the unpleasantness of pain has been a recent topic of interest in both philosophy and neuroscience. Take philosophy first. Although there has lately been a proliferation of subtly different theories, two broad strategies stand out.<a href="#fn:35" id="fnref:35" title="see footnote" class="footnote"><sup>35</sup></a> There are <em>desire-theoretic</em> accounts of pain’s unpleasantness, and there are <em>evaluative</em> accounts of pain’s unpleasantness. According to most desire-theoretic accounts of pain’s unpleasantness, a pain’s unpleasantness consists in the pain-bearer having an intrinsic desire that the pain not occur. According to many evaluative accounts of pain’s unpleasantness, a pain’s unpleasantness consists in the pain representing that the bodily damage that the pain represents is bad for you. There’s a lot to unpack in those definitions, but for our purposes the only important aspect to note is that both broad strategies invoke second-order thoughts: in the one instance, a second-order desire; in the other, a second-order representation. It seems unlikely that cognitively unsophisticated reptiles, amphibians, and fish&#8211;to say nothing of most invertebrates&#8211;are capable of entertaining second-order thoughts.<a href="#fn:36" id="fnref:36" title="see footnote" class="footnote"><sup>36</sup></a></p>

<p>Perhaps, however, investigating the unpleasantness of pain is better conceived as an empirical matter. In that case, we should turn to the neuroscience. Here, again, we find difficulties. Scientists are beginning to suspect there are two functionally distinct pain pathways, the lateral and the medial.<a href="#fn:37" id="fnref:37" title="see footnote" class="footnote"><sup>37</sup></a> The lateral pathway is responsible for representing the intensity of the pain, the location of the pain, and the modality of the pain.<a href="#fn:38" id="fnref:38" title="see footnote" class="footnote"><sup>38</sup></a> The medial pathway represents the degree of unpleasantness of the pain. Importantly, the medial pathway is mediated by the anterior cingulate cortex, a part of the neocortex, which, as we’ve already seen, is unique to mammals. So here again we have some evidence that non-mammalian animals do not experience morally significant pain.</p>

<p>Again, however, the picture is complicated. Pain is a very effective teaching tool. (Indeed, this appears to be the evolutionary role of pain.) Studies show that rats and monkeys with damaged anterior cingulate cortices display almost none of the typical pain-learning behaviors of their undamaged conspecifics. It seems that it is <em>unpleasant</em> pain that is the effective teaching tool. If non-mammalian animals exhibit many of the same pain-learning behaviors as mammals&#8211;and there is good reason to think that <a href="https://doi.org/10.1002/wcs.1248">they do</a>&#8211;then that is some evidence that they are capable of experiencing the unpleasantness of pain. Once again, we can’t rule out the possibility that there are homologous brain structures at work representing the felt badness of pain.</p>

<h1 id="thephenomenalintensityofpain"><strong>The Phenomenal Intensity of Pain</strong></h1>

<p>Some pains <em>hurt</em> more than others. Call this dimension the phenomenal intensity of pain. Ceteris paribus, the greater the phenomenal intensity of a pain, the greater its moral significance. If some nonhumans do experience pain, how intense might their pain be?</p>

<p>The first thing to note is that reported phenomenal intensities of pain, as studied in humans, correlate very poorly with external factors.<a href="#fn:39" id="fnref:39" title="see footnote" class="footnote"><sup>39</sup></a> Even under optimal conditions, a small increase in voltage or temperature can double the reported phenomenal intensity of electric shock or heat-induced stimulus. Indeed, phenomenal intensity can be systematically manipulated completely independently of external stimuli, via <a href="https://doi.org/10.1016/S0304-3959(99)00048-2">hypnotic suggestion</a>. On the other hand, the phenomenal intensity of human pain correlates almost perfectly with the <a href="https://doi.org/10.1152/jn.2001.86.3.1499">firing rates of neurons</a> in the parts of the brain involved in the specific type of pain. If we could get a handle on homologous firing rates in nonhuman animals, we might have a better idea of the intensity of their pain.<a href="#fn:40" id="fnref:40" title="see footnote" class="footnote"><sup>40</sup></a></p>

<p>Another way to potentially get a handle on the phenomenal intensity of nonhuman pain is to consider again the evolutionary role that pain plays. Pain teaches us which stimuli are noxious, how to avoid those stimuli, and what we ought to do to recover from injury. Because intense pain can be distracting, animals in intense pain are at a selective disadvantage compared to conspecifics not in intense pain. Thus, we might expect evolution to select for creatures with pains just phenomenally intense enough (on average) to play the primary instructive role of pain. Humans are the most cognitively sophisticated animals on the planet, the animals most likely to pick up on patterns in signals only weakly conveyed. Less cognitively sophisticated animals generally require stronger signals for pattern-learning. If pain is the signal, then we might reasonably expect the phenomenal intensity of pain to correlate inversely with cognitive sophistication. If that’s the case, humans experience (on average) the <em>least</em> intense pain in all the animal kingdom.<a href="#fn:41" id="fnref:41" title="see footnote" class="footnote"><sup>41</sup></a></p>

<p>A final consideration involves not the phenomenal intensity of pain but its phenomenal <em>extension</em> (that is, its felt duration). Due to neurological differences, phenomenal extension might not be directly comparable across species. Consider brain-processing speed and rates of subjective experience, both loosely defined. Animals with faster metabolisms and smaller body sizes tend, according to <a href="https://doi.org/10.1016/j.anbehav.2013.06.018">some metrics</a>, to process information faster. Thus, there is some reason to think that smaller animals have, in general, <a href="https://reducing-suffering.org/small-animals-clock-speed/">faster subjective experiences</a>. So a hummingbird might experience one minute of objective time<a href="#fn:42" id="fnref:42" title="see footnote" class="footnote"><sup>42</sup></a> as <em>longer</em>, in some robust, non-subjective sense of the term, than a human would. If that’s true, then a given hummingbird and a given human experiencing a pain of the same phenomenal intensity would not, ceteris paribus, suffer equally during the same objective span of time. The hummingbird would suffer <em>more</em>. Hence, we should not naively equate the phenomenal extension of pain with its duration expressed in objective time. The takeaway here is that the moral significance of pain might be related in important ways to an entity’s processing speed. Such concerns would increase exponentially if we ever created <a href="https://doi.org/10.1017/CBO9781139046855.020">artificial minds capable of conscious experience</a>. As with other areas, more research is needed.</p>

<h1 id="degreesofconsciousness"><strong>Degrees of Consciousness</strong></h1>

<p>The moral significance of pain might also depend on the extent to which an entity is aware of (the unpleasantness of) the pain it is in. This is a subtle claim which requires some unpacking. First, distinguish <em>aware of</em> from <em>aware that</em>. I’m not here asserting that the moral significance of pain requires that a pain-bearer be aware <em>that</em> it is in pain.<a href="#fn:43" id="fnref:43" title="see footnote" class="footnote"><sup>43</sup></a> To be aware <em>that</em> one is in pain, one must possess the concept <em>pain</em>. It seems plausible that a creature might experience pain without possessing the concept <em>pain</em>. The extent to which one can be aware <em>of</em> a pain is the extent to which one can <em>attend</em> to a pain. It is the extent to which one is <em>conscious</em> of a pain. And if consciousness comes in degrees, as many neuroscientists believe,<a href="#fn:44" id="fnref:44" title="see footnote" class="footnote"><sup>44</sup></a> then the extent to which one can be aware of pain also comes in degrees, potentially in a morally significant way.</p>

<p>There are several mundane ways in which consciousness can be said to come in degrees. An entity that is conscious might be conscious all the time or only part of the time. (Humans, for example, are unconscious during dreamless sleep and when they undergo general anaesthesia.) For an entity that is currently conscious, consciousness might span many or few modalities. (Some creatures are sensitive to differences in light, sound, temperature, pressure, smell, bodily orientation, and magnetic field. Other creatures are sensitive to fewer sensory modalities.) For an entity that is currently conscious of a given sensory modality, that modality might be coarse-grained or fine-grained. (Within the light modality, some creatures are only sensitive to differences in brightness, while other creatures are sensitive to a wide swath of the electromagnetic spectrum.)</p>

<p>There is a more fundamental sense in which it might be true that consciousness comes in degrees. One of the most striking features of consciousness is its <a href="https://plato.stanford.edu/entries/consciousness-unity/">unity</a>. When I step outside my door, I experience the hum of distant machinery, the gray haze of fog, and the smell of fresh cut grass as elements of a unified and densely integrated representation of reality. Sounds, sights, and smells are all experienced as part of the same <a href="https://plato.stanford.edu/entries/consciousness-neuroscience/#GlobNeurWork">global workspace</a>. This sort of integrated representation may provide for more open-ended behavioral responses than a comparable amount of information presented in isolated streams. If that’s true, then one of the evolutionary functions of consciousness may be to integrate information.</p>

<p>According to the <a href="https://doi.org/10.1371/journal.pcbi.1003588">Integrated Information Theory</a> of consciousness, consciousness just <em>is</em> suitably integrated information. When the effective informational content of a system, mathematically defined in light of the system’s causal profile, is greater than the sum of the informational content of its parts, the system is said to carry <em>integrated information</em>. Integrated information of the relevant source is conscious, whether that integration occurs in the brain or in a two-dimensional graph. Because integration comes in degrees, so too does consciousness.</p>

<p>Intuitively, we might think that creatures like cockroaches and octopuses integrate information to a lesser degree than humans.<a href="#fn:45" id="fnref:45" title="see footnote" class="footnote"><sup>45</sup></a> Headless cockroaches, for example, can be <a href="https://doi.org/10.1098/rspb.1962.0061">trained to avoid electric shocks</a>. Octopuses trained to discriminate between horizontal and vertical rectangles using only one eye <a href="http://psycnet.apa.org/doi/10.1037/h0040366">were unable to discriminate between the shapes using the other eye</a>.<a href="#fn:46" id="fnref:46" title="see footnote" class="footnote"><sup>46</sup></a> One natural interpretation of these results is that although cockroaches and octopuses are adept at detecting and responding to various stimuli, the degree to which that information is centrally processed is limited, at least compared to humans.</p>

<p>If a theory of this sort is correct&#8211;and Integrated Information Theory is often considered <a href="https://www.nytimes.com/2010/09/21/science/21consciousness.html">the leading scientific theory of consciousness</a>&#8211;then different entities will possess different <em>amounts</em> of consciousness. Although it is unclear what a claim of this sort <a href="https://blogs.scientificamerican.com/cross-check/consciousness-and-crazyism-responses-to-critique-of-integrated-information-theory/">even means</a>, it is plausible that the moral significance of pain will depend in part on the amount of consciousness that the entity undergoing the pain possesses.</p>

<h1 id="moraldignityandpain"><strong>Moral Dignity and Pain</strong></h1>

<p><a href="https://plato.stanford.edu/entries/hedonism/">Hedonism</a> is the view (roughly) that the only things that matter morally are pains and pleasures.<a href="#fn:47" id="fnref:47" title="see footnote" class="footnote"><sup>47</sup></a> If hedonism is true, then the (unpleasant) pains and (pleasant) pleasures of nonhumans matter according to their phenomenal intensities and the extent to which the creatures are aware of them. But if hedonism is false, then then there may be reasons to regard those nonhuman pains as less morally significant than human pains.<a href="#fn:48" id="fnref:48" title="see footnote" class="footnote"><sup>48</sup></a> Even if some nonhuman experiences the unpleasantness of pain to the same phenomenal intensity and with the same awareness as a neurotypical adult human, there still might be some difference between the nonhuman and the human which mitigates the moral significance of the nonhuman’s pain.</p>

<p>Let us take just one example.<a href="#fn:49" id="fnref:49" title="see footnote" class="footnote"><sup>49</sup></a> Personal autonomy is the ability to, in some sense, govern oneself. Autonomous agents live their lives according to reasons that are their own, and they act according to motivations largely free from distorting external forces. Autonomous agents possess the capacity to reflectively endorse their commitments and change those commitments when they are found to be deficient. The value of personal autonomy features prominently in much of <a href="https://plato.stanford.edu/entries/autonomy-moral/">modern Western ethics</a>, and it famously was given central place in <a href="https://plato.stanford.edu/entries/kant-moral/#Aut">Immanuel Kant’s moral philosophy</a>. If personal autonomy is non-instrumentally valuable, we might rate the pain of autonomous agents as worse, ceteris paribus, than the pain of non-autonomous entities, especially if the pain interferes somehow with the agent’s autonomy. Because personal autonomy requires self-reflection, many nonhuman animals are not plausible candidates for instantiating this value.<a href="#fn:50" id="fnref:50" title="see footnote" class="footnote"><sup>50</sup></a> Thus, ceteris paribus, their pain may matter less.</p>

<p>Because ethical theorizing is so hard, we should partition our credences over a fairly wide range of plausible normative theories.<a href="#fn:51" id="fnref:51" title="see footnote" class="footnote"><sup>51</sup></a> This partition need not be equal, but it should assign some non-negligible credence even to views strongly at odds with one’s preferred theory. No one ought to be certain, even in the mere colloquial sense of ‘certain,’ that consequentialism or deontology is false.</p>

<h1 id="reflectiveequilibrium"><strong>Reflective Equilibrium</strong></h1>

<p>All ethical theorizing involves some degree of reflective equilibrium. We have intuitions about particular cases and also intuitions about general principles. When we formulate a general principle, we try to capture as many case intuitions as we can. Sometimes, if we are confident in a general principle, we are willing to adjust our judgments in individual cases. Other times, however, our individual judgments are strong enough that they constitute counterexamples to the general principle.<a href="#fn:52" id="fnref:52" title="see footnote" class="footnote"><sup>52</sup></a></p>

<p>When our intuitions about case judgments conflict with our intuitions about general principles, we must decide which to privilege and to what degree. According to the terminology of Roderick Chisholm (1973), the philosophical <em>particularist</em> privileges case judgments over general principles when engaging in reflective equilibrium. The philosophical <em>methodist</em> privileges general principles over case judgments when engaging in reflective equilibrium.<a href="#fn:53" id="fnref:53" title="see footnote" class="footnote"><sup>53</sup></a></p>

<p>Let’s explore a potential conflict. Suppose you believe that the conscious experience of (unpleasant) pain is always morally significant, at least to a small degree. This is a general principle. Suppose you also believe that given the choice between the life of a human child and the lives of a trillion ants, the morally correct action, ceteris paribus, is to save the human child. This is a case judgment. Next, suppose you come to assign, on the basis of solid empirical and philosophical evidence, a small but non-negligible chance to the proposition that ants experience morally significant pain. Because of <a href="https://doi.org/10.1073/pnas.011513798">the sheer number of ants</a>, the amount of expected ant suffering in the world will be quite high. Ameliorating ant suffering suddenly looks like one of the most important issues in the world. This, to say the least, is a surprising result.</p>

<p>How, if at all, should you revise your judgment about whether to save the trillion ants or the single human child? If you do revise your judgment, can you provide an error theory for why the initial judgment was mistaken? If you don’t revise your judgment, does that undercut the general principle? Should you abandon your principle? Or maybe refine it? (Perhaps the aggregation of pain does not consist of mere addition. Or perhaps relatively small instances of pain never sum to relatively and sufficiently big ones.)</p>

<p>Some people may regard it as obvious that one should revise one’s initial case judgment in light of the new information about ant consciousness. Perhaps, but one ought also to be careful not to be pushed down frictionless slopes without proper backstops in place. Here we begin to approach “<a href="https://doi.org/10.1093/analys/anp062">Pascal’s Mugging</a>” territory. For instance: should one assign a non-zero credence to the proposition that <em>plants</em> feel pain? Probably. After all, <a href="https://plato.stanford.edu/entries/panpsychism/">panpsychism</a> might be true. But there are far, <em>far</em> more plants than ants. Even with an <em>extremely</em> low credence that plants experience pain (and I’ll remind you that some <a href="https://en.wikipedia.org/wiki/Galen_Strawson">very smart people endorse panpsychism</a>), expected plant suffering will probably dominate expected ant suffering by several orders of magnitude. Now it looks like ameliorating <em>plant</em> suffering is the most important issue in the world.<a href="#fn:54" id="fnref:54" title="see footnote" class="footnote"><sup>54</sup></a></p>

<p>It’s true that we could continue to adjust our credences downward until we avoid this result, but that somehow feels like cheating. After all, credences are just something we <em>have</em>; they are not the sort of thing we get to set directly. One might reply: “I don’t have infallible epistemic access to all my credences. I know that potential animal suffering is more important than potential plant suffering. I use this information to infer that my credence must be however low it must be in order to avoid the result that expected plant suffering is greater than expected animal suffering.”</p>

<p>This response succeeds up to a point, but ultimately it is unsatisfying. Suppose we discover that we undercounted the number of plants by some 100 quadrillion. (After all, what counts as a “plant” is a somewhat slippery notion.) Then one would have to adjust one’s credence again. At some point these adjustments begin to look ad hoc. A better description of what’s going on looks like this: there are some propositions the entailment of which serve as a <em>reductio ad absurdum</em> on the theory that entails them. <em>That plant suffering matters more than animal suffering</em> is one such proposition. But if we can use the plant-suffering proposition as a reductio on the theory which entails it, why can’t we use the <em>ant</em>-suffering proposition as a reductio on the theory which entails <em>it</em>. After all, didn’t we start with a strong intuition that a trillion ant lives are no more important than a single human life?</p>

<p>The general point here is not that any particular proposition about suffering is absurd or that we should begin our ethical theorizing with any particularly strong views on the worth of ant-lives versus human-lives. The only point I’m trying to make is that bringing one’s theory into reflective equilibrium can be <em>hard</em>. Sometimes there is simply no non-question-begging method to persuade an interlocutor that the equilibrium <em>she</em> has settled on is worse than the equilibrium <em>you</em> have settled on.</p>

<h1 id="directionsforfuturework"><strong>Directions for Future Work</strong></h1>

<p>To recap: I’ve discussed eight conceptually sequential steps needed to identify morally significant pain in nonhumans. The eight steps are:</p>

<ol>
<li>Determine that other minds exist.</li>
<li>Check to see if the nonhuman entity in question engages in pain behavior. If so, check to see if there are any defeaters for the explanation that the entity in question feels pain.</li>
<li>Apply one’s best theory of consciousness to see what it says about the likelihood that the entity in question feels pain.</li>
<li>Assuming that the entity feels pain, check to see if it experiences the felt badness of pain.</li>
<li>Determine the phenomenal intensity and phenomenal extension of the pain.</li>
<li>Determine the degree to which the entity is aware of the pain.</li>
<li>Determine the entity’s moral standing relative to other entities which experience pain.</li>
<li>Check to see if your final result constitutes a reductio on the whole process.</li>
</ol>

<p>There is a tremendous amount of uncertainty, both empirical and moral, surrounding the issue of nonhuman pain. Because the subject is so complex, we should ascribe some credence to views which hold that phenomenal consciousness is rare outside humans and also ascribe some credence to views which hold that phenomenal consciousness, though common, is not terribly morally significant.</p>

<p>Nonetheless, my personal view is that even after folding all this uncertainty into our calculations, we are still left with the result that we should take nonhuman pain much more seriously than the average policymaker does. There are good reasons to think that many nonhumans feel pain and that this pain is morally significant. These nonhumans do not have a voice in policy debate and they do not have a vote. They are powerless to stop the harms we inflict on them, and they are powerless to ask us for help. They are not just systematically mistreated; their suffering is almost wholly ignored.</p>

<p>One of the best ways to help these creatures is to reduce the uncertainties surrounding the issue of nonhuman pain. To that end, Rethink Priorities has been working on an ambitious project to analyze and catalogue 60+ features potentially relevant to phenomenal consciousness and morally significant pain. (A project of this sort <a href="https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood#GuessingProjects">was suggested</a> by Luke Muehlhauser in his <a href="https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood">2017 Report on Consciousness and Moral Patienthood</a>.) We aim to carefully define each feature and explain why and to what degree it might be relevant to consciousness. We have selected 17 representative species from across the animal kingdom and are currently scouring the scientific literature to see whether and to what extent each species exhibits each of the features. Some of the species are intuitively conscious (e.g., cows), while others are intuitively not (e.g., nematodes). In between are a host of interesting edge cases, like honey bees and octopuses. All this information will eventually be compiled into an easily searchable database. Of course, the project won’t definitively settle whether honey bees or octopuses experience morally significant pain. Nonetheless, our hope is that the database will become an invaluable resource for future consciousness research. In our next essay, we explain this approach in more detail.</p>

<h1 id="acknowledgments"><strong>Acknowledgments</strong></h1>

<p>J.P. Andrew, Elijah Armstrong, Kim Cuddington, Marcus A. Davis, Neil Dullaghan, Sam Fox Krauss, Peter Hurford, David Moss, Katie Plemmons, and Daniela R. Waldhorn provided helpful comments on this essay.</p>

<h1 id="endnotes"><strong>Endnotes</strong></h1>

<div class="footnotes">
<hr />
<ol>

<li id="fn:1">
<p>As we’ll see, it’s not enough to demonstrate that nonhumans experience pain. There are a number of ways in which nonhuman pain might be less morally significant than human pain, even to the point that nonhuman pain fails to be morally significant at all. Nonhuman pain might just <em>feel</em> different (along dimensions elaborated below) in a way which renders the pain less morally pressing. <a href="#fnref:1" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:2">
<p>To take just one group of arthropods, there are something like a <em>quintillion</em> insects alive at any given moment, a number which boggles the mind. See C.B. Williams. 1964. <em>Patterns in the Balance of Nature and Related Problems in Quantitative Biology</em>. Academic Press, London: 324. <a href="#fnref:2" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:3">
<p>Elasmobranch fish (i.e., cartilaginous fish, such as sharks) may be an exception. See, inter alia, Ewan Smith and Gary Lewin. 2009. “<a href="https://doi.org/10.1007/s00359-009-0482-z">Nociceptors: A Phylogenetic View</a>.” <em>Journal of Comparative Physiology A</em> Vol. 195, Issue 12: 1096. <a href="#fnref:3" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:4">
<p>For comparison, we also include some non&#8211;animals, such as plants and <a href="https://www.aaas.org/problem-protists">protists</a>. <a href="#fnref:4" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:5">
<p>This list is not exhaustive. Most notably, we’ll set aside difficult questions in metaethics. For example, if moral nihilism is true, then there are no moral facts, and thus no creatures experience morally significant pain, including humans. <a href="#fnref:5" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:6">
<p>Note that not every step is equally problematic. <a href="#fnref:6" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:7">
<p>I here set aside certain conceptually possible but non&#8211;actual fanciful devices, such as brain&#8211;to&#8211;brain hookups. <a href="#fnref:7" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:8">
<p>This is metaphysical solipsism, and the description is not technically correct (it’s a necessary but not sufficient part of the full view). One could believe that one’s mind is the only one which exists without thereby being a solipsist, if, say, one were the sole survivor of some apocalyptic catastrophe. <a href="#fnref:8" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:9">
<p>The ammunition metaphor is adapted from Anil Gupta. 2006. <em><a href="https://doi.org/10.1093/acprof:oso/9780195189582.001.0001">Empiricism and Experience</a></em>. Oxford University Press: 178. <a href="#fnref:9" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:10">
<p>See, inter alia, Andrew Melnyk. 1994. “<a href="https://doi.org/10.1080/00048409412346281">Inference to the Best Explanation and Other Minds</a>.” <em>Australasian Journal of Philosophy</em>, 72: 482–91 for a discussion of the issue. <a href="#fnref:10" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:11">
<p>Obviously, this is a simplification. The behavioral similarities run much deeper. <a href="#fnref:11" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:12">
<p>Other explanations are more complicated because they raise more questions than they resolve. Why, for instance, would someone create sophisticated robots programmed to behave as I do? <a href="#fnref:12" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:13">
<p>It’s important to note that one can <em>prefer</em> an explanation without fully <em>believing</em> the explanation. If there are numerous plausible explanations, the best explanation might only warrant a credence of .2. For example, it’s consistent to have a fairly low credence in the claim that invertebrates feel pain and yet think that that explanation of their behavior is more likely than any other explanation of their behavior. See Michael Tye. 2017. <em><a href="https://global.oup.com/academic/product/tense-bees-and-shell-shocked-crabs-9780190278014?cc=us&amp;lang=en&amp;">Tense Bees and Shell&#8211;Shocked Crabs</a></em>. Oxford University Press: 68. <a href="#fnref:13" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:14">
<p>To put the point another way, ancient hunter&#8211;gatherers were justified in believing their fellow humans felt pain, but they didn't know anything about physiological or neurological similarity. See Tye 2017: 53&#8211;56 for more on this point. <a href="#fnref:14" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:15">
<p>This way of formulating the justificatory base leads to the well&#8211;known <a href="https://plato.stanford.edu/entries/induction-problem/">problem of induction</a>, which I here gently set aside. <a href="#fnref:15" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:16">
<p>See, e.g., Tye 2017, especially chapter 5. <a href="#fnref:16" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:17">
<p>Here, “pain behavior” doesn’t mean “behavior caused by pain.” Rather, it is convenient shorthand for “behavioral patterns, that, <em>in humans</em>, are caused by pain.” <a href="#fnref:17" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:18">
<p>This is not an ad hoc view. Functional imaging studies show that, in humans, there is a correlation between the phenomenal intensity of pain and activity in the anterior cingulate cortex and the somatosensory cortex. See Devinsky, O., Morrell, M. J., &amp; Vogt, B. A. 1995. “<a href="https://doi.org/10.1093/brain/118.1.279">Contributions of Anterior Cingulate Cortex to Behaviour</a>.” <em>Brain: A Journal of Neurology</em>, 118(1), 279&#8211;306. <a href="#fnref:18" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:19">
<p>The neocortex is only found in mammalian brains. <a href="#fnref:19" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:20">
<p>See Merker, B. 2007. “<a href="https://doi.org/10.1017/S0140525X07000891">Consciousness without a Cerebral Cortex: A challenge for Neuroscience and Medicine</a>.” <em>Behavioral and Brain Sciences</em>, 30(1), 63&#8211;81. It should be noted that this claim only applies to children born without a neocortex. Adults with damaged neocortices remain completely vegetative. <a href="#fnref:20" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:21">
<p>Jarvis ED, Güntürkün O, Bruce L, Csillag A, Karten H, Kuenzel W, et al. 2005. &#8220;<a href="https://doi.org/10.1038/nrn1606">Avian brains and a new understanding of vertebrate brain evolution</a>.&#8221; <em>Nature Reviews. Neuroscience</em>. 6 (2): 151–9. See Tye 2017: 78&#8211;84 for a philosophical discussion. <a href="#fnref:21" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:22">
<p>This is actually fairly rare in philosophy. Epistemologists and ethicists often develop radically different theories on the basis of roughly the same common ground. The only other comparable example that comes to mind in philosophy is <a href="https://plato.stanford.edu/entries/mereology/">mereology</a>. <a href="#fnref:22" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:23">
<p>See Schwitzgebel (forthcoming) “<a href="http://www.faculty.ucr.edu/~eschwitz/SchwitzAbs/Snails.htm">Is There Something It Is Like to Be a Garden Snail</a>” for more on the common ground problem. <a href="#fnref:23" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:24">
<p>An alternate explanation holds that the panpsychist and higher&#8211;order theorist begin with the same starting assumptions, but the theoretical virtues of their respective theories lead them to remarkably different conclusions. I am dubious of this explanation. <a href="#fnref:24" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:25">
<p>Thomas Huxley is perhaps the most famous philosophical proponent of epiphenomenalism. See his (1874) “<a href="https://www.jstor.org/stable/27793698">On the Hypothesis that Animals Are Automata</a>.” <em>Victorian Review</em> Vol. 35, No. 1, pp. 50&#8211;52. <a href="#fnref:25" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:26">
<p>This is too quick. In <a href="https://plato.stanford.edu/entries/decision-causal/#NewcProb">weird cases</a> an action can be evidence for some state of affairs without bearing any causal relationship to that state of affairs. But the general in&#8211;text point stands. <a href="#fnref:26" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:27">
<p>See David Chalmers. 1996. <em><a href="https://global.oup.com/ushe/product/the-conscious-mind-9780195117899?cc=us&amp;lang=en&amp;">The Conscious Mind</a></em>. Oxford University Press: pp. 94&#8211;99 for the canonical discussion. Note also that Chalmers hedges on whether his view entails true epiphenomenalism. He admits his view entails “<em>something like</em> epiphenomenalism” (150, emphasis in the original). <a href="#fnref:27" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:28">
<p><em>Ibid</em>. Although phenomenal zombies get a lot of press, they are, according to Chalmers at least, inessential to his broader arguments. <a href="#fnref:28" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:29">
<p>The exception is so&#8211;called “interactionist dualism.” Descartes is probably the most famous interactionist dualist. He believed that nonphysical mental states affect the brain <a href="https://plato.stanford.edu/entries/pineal-gland/">via the pineal gland</a>. <a href="#fnref:29" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:30">
<p>See C.D. Broad. 1925. <em><a href="https://www.amazon.com/Mind-Place-Nature-Phil-osophy-Language/dp/0415488257">The Mind and Its Place in Nature</a></em>. London: Kegan Paul: 125. <a href="#fnref:30" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:31">
<p>Carl G. Hempel and Paul Oppenheim. 1948. &#8220;<a href="https://doi.org/10.1086/286983">Studies in the Logic of Explanation</a>.&#8221; <em>Philosophy of Science</em> 15, no. 2 (Apr., 1948): 119. <a href="#fnref:31" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:32">
<p>Even if the features <em>were</em> coextensive in the animal kingdom, it seems physically possible to deliberately design an entity which possessed some but not all of the features. <a href="#fnref:32" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:33">
<p>It is important to emphasize that, as best we can tell, pain asymbolia patients experience something over and above mere <a href="https://www.jci.org/articles/view/42843">nociception</a>. Nociceptors are special receptors used by the body to detect potentially harmful stimuli. Many creatures, including, for example, the roundworm <em>C. elegans</em>, <a href="https://doi.org/10.1152/physiol.00022.2017">possess nociceptors</a>. Mere nociception does not have an attendant phenomenology, whereas pain asymbolia patients do report a conscious experience&#8211;just not an unpleasant one. <a href="#fnref:33" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:34">
<p>There are many ways in which pain experiences are <em>instrumentally good</em>: they alert us to potential damage, they aid in our recovery from such damage, and they enable us to learn to avoid such damage in the future. We shouldn’t wish to be completely without pain, for humans born in such a condition (known as general congenital analgesia) almost always <a href="https://ghr.nlm.nih.gov/condition/congenital-insensitivity-to-pain">die young</a>. <a href="#fnref:34" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:35">
<p>See David Bain. 2017. “<a href="https://doi.org/10.1111/nous.12228">Why Take Painkillers?</a>” <em>Nous</em> for a recent representative entry in the debate. <a href="#fnref:35" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:36">
<p>Of course, we might be overestimating the cognitive sophistication required for second&#8211;order mental states, especially if we drop the assumption that higher&#8211;order cognition must be propositional. <a href="#fnref:36" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:37">
<p>See Adam Shriver. 2006. “<a href="https://doi.org/10.1080/09515080600726385">Minding Mammals</a>.” <em>Philosophical Psychology</em> Vol. 19: 433&#8211;442 (especially sec. 2) for an accessible overview. <a href="#fnref:37" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:38">
<p>“The modality of the pain” refers to the type of pain (e.g., a “cutting” pain, a “burning” pain, a “throbbing” pain). <a href="#fnref:38" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:39">
<p>See Adam Pautz. 2014. “<a href="https://doi.org/10.1007/978-94-007-6001-1_18">The Real Trouble with Phenomenal Externalism: New Empirical Evidence for a Brain&#8211;Based Theory of Consciousness.</a>” in: Brown R. (eds) <em>Consciousness Inside and Out: Phenomenology, Neuroscience, and the Nature of Experience. Studies in Brain and Mind</em>, vol 6. Springer, Dordrecht, especially sec 2.3 for an overview. <a href="#fnref:39" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:40">
<p>There has been some initial research along these lines in <a href="https://doi.org/10.1152/jn.2000.84.2.719">monkeys</a>, but, perhaps for obvious reasons, the subject is not widely studied. <a href="#fnref:40" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:41">
<p>It should be noted that, more so than other sections, this paragraph is entirely speculative. <a href="#fnref:41" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:42">
<p>For the sticklers out there, I am well aware that Einstein taught us there is no good sense to the term “objective time.” I of course assume here that the hummingbird and the human are in the same reference frame. <a href="#fnref:42" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:43">
<p>Of course, certain second&#8211;order thoughts about pain might themselves be morally significant. A human who is aware <em>that</em> she is in pain might also come to believe that the pain is just or unjust. <a href="#fnref:43" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:44">
<p>For the record, most philosophers disagree. <a href="#fnref:44" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:45">
<p>It should be stressed that according to Integrated Information Theory, information integration is defined mathematically, so these intuitive examples may not stand up to greater scrutiny. <a href="#fnref:45" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:46">
<p>Failures of so&#8211;called “inter&#8211;ocular transfer” have also been found in birds, fish, reptiles and amphibians. See G Vallortigara, L.J Rogers, A Bisazza. “<a href="https://doi.org/10.1016/S0165-0173(99)00012-0">Possible evolutionary origins of cognitive brain lateralization</a>.” <em>Brain Research Reviews</em> Vol. 30: 164&#8211;175 for a scientific discussion. See Peter Godfrey&#8211;Smith. 2016. <em><a href="https://www.amazon.com/Other-Minds-Octopus-Origins-Consciousness/dp/0374227764">Other Minds: The Octopus, The Sea, and the Deep Origins of Consciousness</a></em>. New York: Farrar, Straus and Giroux: 84&#8211;87 for a philosophical discussion. <a href="#fnref:46" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:47">
<p>This is ethical hedonism. Psychological hedonism is the (descriptive) view that only pleasures and pain motivate us. <a href="#fnref:47" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:48">
<p>It should be emphasized that it does not follow from the falsity of hedonism that pains and pleasures are morally insignificant. If hedonism is false, it is almost certainly because there are <em>other</em> things which are valuable as well. <a href="#fnref:48" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:49">
<p>Additional examples could be drawn from certain theistic traditions in which God (allegedly) gives humans dominion over (other) animals. <a href="#fnref:49" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:50">
<p>On this subject Kant writes: “The fact that the human being can have the representation ‘I’ raises him infinitely above all the other beings on earth. By this he is a person.. that is, a being altogether different in rank and dignity from things, such as irrational animals, with which one may deal and dispose at one’s discretion.” “Anthropology from a Pragmatic Point of View (1798)” in 2007. <em><a href="https://doi.org/10.1017/CBO9780511791925">Anthropology, History, and Education</a></em>. (Cambridge Edition of the Works of Immanuel Kant). Robert Louden and Gunter Zoller (eds. and trans.). Cambridge University Press: 239. Some claim that Kant drew the wrong inference from his own theory. See Christine Korsgaard. 2018. <em><a href="https://global.oup.com/academic/product/fellow-creatures-9780198753858?cc=us&amp;lang=en&amp;">Fellow Creatures: Our Obligations to Other Animals</a></em>. Oxford University Press, especially Part Two: “Immanuel Kant and the Animals.” <a href="#fnref:50" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:51">
<p>See, inter alia, Will MacAskill. 2016. “<a href="https://doi.org/10.1093/mind/fzv169">Normative Uncertainty as a Voting Problem</a>.” <em>Mind</em>, Vol. 125: 967&#8211;1004 for more on normative uncertainty. <a href="#fnref:51" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:52">
<p>In the words of Nelson Goodman: “[R]ules and particular inferences alike are justified by being brought into agreement with each other. A rule is amended if it yields an inference we are unwilling to accept; an inference is rejected if it violates a rule we are unwilling to amend.” <em><a href="http://www.hup.harvard.edu/catalog.php?isbn=9780674290716">Fact, Fiction, and Forecast</a></em>. Harvard University Press (1955): 61&#8211;2. <a href="#fnref:52" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:53">
<p><em><a href="https://www.amazon.com/Problem-Criterion-Aquinas-Lecture-1973/dp/0874621380">The Problem of the Criterion</a></em>. Marquette University Press: 15. <a href="#fnref:53" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

<li id="fn:54">
<p>Some members of the effective altruism community go further, positing that “atomic movements, electron orbits, photon collisions, etc. could <a href="https://reducing-suffering.org/is-there-suffering-in-fundamental-physics/">collectively deserve significant moral weight</a>.” <a href="#fnref:54" title="return to body" class="reversefootnote">&#160;&#8617;</a></p>
</li>

</ol>
</div>
